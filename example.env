# RENAME THIS FILE TO .env

# ######## #
# Weaviate #
# ######## #

# WEAVIATE_HOST='weaviate'
# WEAVIATE_PORT='8080'
# WEAVIATE_SCHEME='http'
# WEAVIATE_GRPC_HOST='weaviate'
# WEAVIATE_GRPC_PORT='50051'
# WEAVIATE_API_KEY='@5D%t$3b49NWx'
# WEAVIATE_REPLICAS='1'
# WEAVIATE_BACKUP_RESTORE_ID=
# WEAVIATE_BACKUP_HOURLY_FREQUENCY='24'

# ##### #
# REDIS #
# ##### #

# REDIS_URL='redis://redis:6379'
# REDIS_PW='596Lf6'
# REDIS_PORT='6379'

# ############## #
# OpenAI service #
# ############## #

# OPENAI_KEY=

# Below variables let you set specific model versions. The defaults are shown below.

# OPEN_AI_TEXT_GEN_MODEL='gpt-4.1'
# OPEN_AI_TEXT_GEN_MINI_MODEL='gpt-4.1-mini'
# OPEN_AI_TEXT_GEN_REASONING_LRG_MODEL='o3'
# OPEN_AI_TEXT_GEN_REASONING_MINI_MODEL='o4-mini'
# OPEN_AI_IMG_GEN_MODEL='gpt-image-1'
# OPEN_AI_EMBEDDING_MODEL='text-embedding-3-small'

# #################### #
# Azure OpenAI service #
# #################### #

# AZURE_OPENAI_API_VERSION='2024-12-01-preview'

# USING_GPT4O='true' # Set this to false only if you are using an older model without vision support.

# Text & embedding generation
# AZURE_OPENAI_KEY=
# AZURE_OPENAI_RESOURCE=
# AZURE_OPENAI_TXT_DEPLOYMENT=
# AZURE_OPENAI_TXT_DEPLOYMENT_MINI=
# AZURE_OPENAI_EMBED_DEPLOYMENT=

# Vision -> This can be the same values as the text and embedding generation variables but still must be specified
# AZURE_OPENAI_VISION_RESOURCE=
# AZURE_OPENAI_VISION_DEPLOYMENT=
# AZURE_OPENAI_VISION_KEY=

# Image
# AZURE_OPENAI_IMG_RESOURCE=
# AZURE_OPENAI_IMG_DEPLOYMENT=
# AZURE_OPENAI_IMG_KEY=

# Realtime
# AZURE_REALTIME_URL=
# AZURE_REALTIME_KEY=

# o1
# AZURE_OPENAI_REASONING_KEY=
# AZURE_OPENAI_REASONING_RESOURCE=
# AZURE_OPENAI_REASONING_DEPLOYMENT=
# AZURE_OPENAI_REASONING_MINI_DEPLOYMENT=

# ################ #
# CLAUDE ANTHROPIC #
# ################ #

# CLAUDE_MODEL=
# CLAUDE_MINI_MODEL=
# CLAUDE_API_KEY=

# ########## #
# CLAUDE AWS #
# ########## #

# CLAUDE_AWS_REGION=
# CLAUDE_AWS_ACCESS_KEY=
# CLAUDE_AWS_SECRET_KEY=

# ###### #
# GEMINI #
# ###### #

# GEMINI_MODEL=
# GEMINI_MINI_MODEL=
# GEMINI_EMBEDDING_MODEL=

# ######### #
# AI Studio #
# ######### #

# GEMINI_API_KEY=

# ########### #
# VERTEX AUTH #
# ########### #

# VERTEX_PROJECT=
# VERTEX_LOCATION=

# ###### #
# Ollama #
# ###### #

# OLLAMA_URL=
# OLLAMA_MODEL=
# OLLAMA_MODEL_MINI=
# OLLAMA_KEEP_ALIVE=
# OLLAMA_EMBEDDING_MODEL=
# OLLAMA_MAX_EMBEDDING_TEXT_TOKENS=

# ############################ #
# Model-Specific Token Defaults #
# ############################ #
# These settings control token limits for different model families.
# The values shown are the defaults used by the application if the environment variables are not set.
# Refer to the README for more details on valid ranges.
# Max conversation token count is the maximum number of tokens that will be sent to a model, this must be less than the model's maximum context window.
# The application automatically calculates the maximum tokens sent to the model to allow for the response tokens, so you do not need to factor in this value. For example, GPT4.1 has a 1,047,576 context window and 32,768 max output tokens (https://platform.openai.com/docs/models/gpt-4.1). You can set validly set MAX_CONVERSATION_TOKEN_COUNT='1047576' and MAX_OUTPUT_TOKENS='32768'.

# For standard OpenAI GPT-4 family models (e.g., gpt-4o, not "mini" or "reasoning" variants)
# MAX_OUTPUT_TOKENS='16384' # Max response tokens (already listed above, effectively for modelDefaults.GPT4)
# MAX_CONVERSATION_TOKEN_COUNT='124000' # Max conversation token (maximum number of tokens that will be sent to a model)

# For "mini" GPT-4 family models (e.g., gpt-4o-mini)
# MAX_OUTPUT_TOKENS_SMALL='16384'
# MAX_CONVERSATION_TOKEN_COUNT_SMALL='124000'

# For large OpenAI reasoning models (e.g., o1, o3)
# reasoning_lrg_openAI_max_response='50000'
# reasoning_lrg_openAI_max_convo_tokens='200000'

# For mini OpenAI reasoning models (e.g., o1-mini)
# reasoning_mini_openAI_max_response='50000'
# reasoning_mini_openAI_max_convo_tokens='120000'

# For Claude models
# claude_max_response_tokens='8192'
# claude_max_conversation_tokens='200000'
# claude_mini_max_response_tokens='8192'
# claude_mini_max_conversation_tokens='200000'

# For Gemini models
# gemini_max_response_tokens='50000'
# gemini_max_conversation_tokens='1000000'
# gemini_mini_max_response_tokens='50000'
# gemini_mini_max_conversation_tokens='1000000'

# For Ollama models
# OLLAMA_MAX_RESPONSE_TOKENS=
# OLLAMA_MAX_CONVERSATION_TOKENS=
# OLLAMA_MINI_MAX_RESPONSE_TOKENS=
# OLLAMA_MINI_MAX_CONVERSATION_TOKENS=

# ############ #
# FILE STORAGE #
# ############ #

# Storage type, can be any of the following: 'azure' | 'aws' | 'google' | 's3-compatible'
# FILE_CLOUD_PROVIDER='s3-compatible'

# Azure storage
# =============

# AZURE_STORAGE_CONNECTION_STRING=
# AZURE_STORAGE_ACCOUNT_NAME=
# AZURE_STORAGE_ACCOUNT_KEY=
# AZURE_CONTAINER_NAME=

# 
# AWS S3 Storage
# ==============

# AWS_REGION=
# AWS_ACCESS_KEY_ID=
# AWS_SECRET_ACCESS_KEY=
# AWS_BUCKET_NAME=

# 
# GOOGLE CLOUD
# ============

# GOOGLE_PROJECT_ID=
# GOOGLE_CLIENT_EMAIL=
# GOOGLE_PRIVATE_KEY=
# GOOGLE_BUCKET_NAME=

# 
# s3 compatible - default Minio
# =============================

# If using Minio or another local s3 service provider, you must set this as localhost (not 0.0.0.0 or 127.0.0.1). Further, when running an s3 provider in Docker as part of the Docker compose setup, you must set the service name in the S3_COMPATIBLE_ENDPOINT_SERVICE env variable -> This is due to networking issues when dealing with services in Docker that need to also be accessed from the web browser. If using cloud-based s3 services, you must set the endpoint to the correct URL and can leave the S3_COMPATIBLE_ENDPOINT_SERVICE commented out.
# S3_COMPATIBLE_ENDPOINT='http://localhost:9000'
# S3_COMPATIBLE_ENDPOINT_SERVICE='minio' 
# S3_COMPATIBLE_ACCESS_KEY_ID='minioadmin'
# S3_COMPATIBLE_SECRET_ACCESS_KEY='minioadmin'
# S3_COMPATIBLE_BUCKET_NAME='academicid'
# S3_COMPATIBLE_FORCE_PATH_STYLE='true'
# S3_COMPATIBLE_REGION='auto'

# ################### #
# Additional Settings #
# ################### #

# What default models to use for background text generation and embedding generation.

# DEFAULT_TEXT_GEN_MODEL='GPT4' # OPTIONS: 'GPT4', 'Gemini', 'Claude', 'Ollama'
# EMBEDDING_PROVIDER='openAI' # OPTIONS: 'openAI', 'google', 'ollama'

# SEMANTIC_SCHOLAR_API_KEY=
# PAPERBUZZ_EMAIL=
# BACKEND_PORT='3030'

## These four variables are used to set the frontend and backend URLs for the application. If you are running the application locally, you can leave them commented out as the default values will be used. If you are deploying the application behind a reverse proxy or otherwise accessing through a different URL, you can uncomment and set these variables to the appropriate values. The NUXT_PUBLIC_API_BASE and NUXT_PUBLIC_WS_BASE variables are used by the Nuxt.js frontend to access the backend API and WebSocket endpoints, respectively.
# FRONTEND_URL='http://localhost:3000'
# BACKEND_URL='http://localhost:3030'
# NUXT_PUBLIC_API_BASE='http://localhost:3030/api'
# NUXT_PUBLIC_WS_BASE='ws://localhost:3030'

# TIMEZONE='UTC'
# VECTOR_LENGTH='1536' # This currently cannot be changed once set.
# CUSTOM_PROMPT=

# Set this to true if you do not want the chat to have access to function calls. This is useful for self-hosted models that either do not support function calling or get easily confused by it.
# NO_CHAT_FUNCTIONS=
 
# MAX_WORKERS='3' # sets the number of files that can be processed in parallel (for both the file processing service and research spaces)
 
# ADVANCED_EXTRACTION= # Only set this if you want to extract text from files using the GPT4o vision capabilities; this results in better text extraction and performance in search and RAG, however, it is more expensive as all text is extracted using the GPT4o vision model

# INTERNAL_APP_COMMUNICATION_PORT='3032' # This is the port used for internal communication between the different backend services. It should not be changed unless you know what you are doing.
# MAIN_APP_URL='https://acid_backend:3032' # This is the url for the services to connect to the main backend application. It should not be changed unless you know what you are doing. 