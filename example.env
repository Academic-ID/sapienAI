# RENAME THIS FILE TO .env

# ######## #
# Weaviate #
# ######## #

# WEAVIATE_HOST='weaviate'
# WEAVIATE_PORT='8080'
# WEAVIATE_SCHEME='http'
# WEAVIATE_GRPC_HOST='weaviate'
# WEAVIATE_GRPC_PORT='50051'
# WEAVIATE_API_KEY='@5D%t$3b49NWx'
# WEAVIATE_REPLICAS='1'
# WEAVIATE_BACKUP_RESTORE_ID=
# WEAVIATE_BACKUP_HOURLY_FREQUENCY='24'

# ##### #
# REDIS #
# ##### #

# REDIS_URL='redis://redis:6379'
# REDIS_PW='596Lf6'
# REDIS_PORT='6379'

# ############## #
# OpenAI service #
# ############## #

# OPENAI_KEY=
# Below variables let you set specific model versions. The defaults are shown below.
# OPEN_AI_TEXT_GEN_MODEL='gpt-4o'
# OPEN_AI_TEXT_GEN_MINI_MODEL='gpt-4o-mini'
# OPEN_AI_TEXT_GEN_REASONING_LRG_MODEL='o1'
# OPEN_AI_TEXT_GEN_REASONING_MINI_MODEL='o1-mini'

# #################### #
# Azure OpenAI service #
# #################### #

# AZURE_OPENAI_API_VERSION='2024-12-01-preview'

# USING_GPT4O='true'

# Text & embedding generation 
# AZURE_OPENAI_KEY=
# AZURE_OPENAI_RESOURCE=
# AZURE_OPENAI_TXT_DEPLOYMENT=
# AZURE_OPENAI_TXT_DEPLOYMENT_MINI=
# AZURE_OPENAI_EMBED_DEPLOYMENT=

# Vision -> This can be the same values as the text and embedding generation variables but still must be specified
# AZURE_OPENAI_VISION_RESOURCE=
# AZURE_OPENAI_VISION_DEPLOYMENT=
# AZURE_OPENAI_VISION_KEY=

# Image
# AZURE_OPENAI_IMG_RESOURCE=
# AZURE_OPENAI_IMG_DEPLOYMENT=
# AZURE_OPENAI_IMG_KEY=

# Realtime
# AZURE_REALTIME_URL=
# AZURE_REALTIME_KEY=

# o1
# AZURE_OPENAI_REASONING_KEY=
# AZURE_OPENAI_REASONING_RESOURCE=
# AZURE_OPENAI_REASONING_DEPLOYMENT=
# AZURE_OPENAI_REASONING_MINI_DEPLOYMENT=

# ################ #
# CLAUDE ANTHROPIC #
# ################ #

# CLAUDE_MODEL=
# CLAUDE_API_KEY=

# ########## #
# CLAUDE AWS #
# ########## #

# CLAUDE_AWS_MODEL=
# CLAUDE_AWS_REGION=
# CLAUDE_AWS_ACCESS_KEY=
# CLAUDE_AWS_SECRET_KEY=

# ###### #
# GEMINI #
# ###### #

# GEMINI_MODEL=
# GEMINI_API_KEY=

# ########### #
# VERTEX AUTH #
# ########### #

# VERTEX_PROJECT=
# VERTEX_LOCATION=

# ############################ #
# Model-Specific Token Defaults #
# ############################ #
# These settings control token limits for different model families.
# The values shown are the defaults used by the application if the environment variables are not set.
# Refer to the README for more details on valid ranges.
# Max conversation token count is the maximum number of tokens that will be sent to a model, this must be less than the model's maximum context window.
# The application automatically calculates the maximum tokens sent to the model to allow for the response tokens, so you do not need to factor in this value. For example, GPT4.1 has a 1,047,576 context window and 32,768 max output tokens (https://platform.openai.com/docs/models/gpt-4.1). You can set validly set MAX_CONVERSATION_TOKEN_COUNT='1047576' and MAX_OUTPUT_TOKENS='32768'.

# For standard GPT-4 family models (e.g., gpt-4o, not "mini" or "reasoning" variants)
# MAX_OUTPUT_TOKENS='16384' # Max response tokens (already listed above, effectively for modelDefaults.GPT4)
# MAX_CONVERSATION_TOKEN_COUNT='124000' # Max conversation token (maximum number of tokens that will be sent to a model)

# For "mini" GPT-4 family models (e.g., gpt-4o-mini)
# MAX_OUTPUT_TOKENS_SMALL='16384'
# MAX_CONVERSATION_TOKEN_COUNT_SMALL='124000'

# For large OpenAI reasoning models (e.g., o1, o3)
# reasoning_lrg_openAI_max_response='100000'
# reasoning_lrg_openAI_max_convo_tokens='200000'

# For mini OpenAI reasoning models (e.g., o1-mini)
# reasoning_mini_openAI_max_response='65000'
# reasoning_mini_openAI_max_convo_tokens='120000'

# For Claude models
# claude_max_response_tokens='8192'
# claude_max_conversation_tokens='200000'

# For Gemini models
# gemini_max_response_tokens='65000'
# gemini_max_conversation_tokens='1000000'

# ############ #
# FILE STORAGE #
# ############ #

# Storage type, can be any of the following: 'azure' | 'aws' | 'google' | 's3-compatible'
# FILE_CLOUD_PROVIDER='s3-compatible'

# 
# Azure storage
# 

# AZURE_STORAGE_CONNECTION_STRING=
# AZURE_STORAGE_ACCOUNT_NAME=
# AZURE_STORAGE_ACCOUNT_KEY=
# AZURE_CONTAINER_NAME=

# 
# AWS S3 Storage
# 

# AWS_REGION=
# AWS_ACCESS_KEY_ID=
# AWS_SECRET_ACCESS_KEY=
# AWS_BUCKET_NAME=

# 
# GOOGLE CLOUD
# 

# GOOGLE_PROJECT_ID=
# GOOGLE_CLIENT_EMAIL=
# GOOGLE_PRIVATE_KEY=
# GOOGLE_BUCKET_NAME=

# 
# s3 compatible - default Minio
# 

# If using Minio or another local s3 service provider, you must set this as localhost (not 0.0.0.0 or 127.0.0.1). Further, when running an s3 provider in Docker as part of the Docker compose setup, you must set the service name in the S3_COMPATIBLE_ENDPOINT_SERVICE env variable -> This is due to networking issues when dealing with services in Docker that need to also be accessed from the web browser. If using cloud-based s3 services, you must set the endpoint to the correct URL and can leave the S3_COMPATIBLE_ENDPOINT_SERVICE commented out.
# S3_COMPATIBLE_ENDPOINT='http://localhost:9000'
# S3_COMPATIBLE_ENDPOINT_SERVICE='minio' 
# S3_COMPATIBLE_ACCESS_KEY_ID='minioadmin'
# S3_COMPATIBLE_SECRET_ACCESS_KEY='minioadmin'
# S3_COMPATIBLE_BUCKET_NAME='academicid'
# S3_COMPATIBLE_FORCE_PATH_STYLE='true'

# ################### #
# Additional Settings #
# ################### #

# SEMANTIC_SCHOLAR_API_KEY=
# PAPERBUZZ_EMAIL=
# BACKEND_PORT='3030'
# FRONTEND_URL='http://localhost:3000'
# BACKEND_URL='http://localhost:3030'
# TIMEZONE='UTC'
# VECTOR_LENGTH='1536' # must be either '1536' or '512'
# CUSTOM_PROMPT=
 
# MAX_WORKERS='3' # sets the number of files that can be processed in parallel (for both the file processing service and research spaces)
 
# ADVANCED_EXTRACTION= # Only set this if you want to extract text from files using the GPT4o vision capabilities; this results in better text extraction and performance in search and RAG, however, it is more expensive as all text is extracted using the GPT4o vision model

