# RENAME THIS FILE TO .env

# ######## #
# Weaviate #
# ######## #

# WEAVIATE_HOST='weaviate'
# WEAVIATE_PORT='8080'
# WEAVIATE_SCHEME='http'
# WEAVIATE_GRPC_HOST='weaviate'
# WEAVIATE_GRPC_PORT='50051'
# WEAVIATE_API_KEY='@5D%t$3b49NWx'
# WEAVIATE_REPLICAS='1'
# WEAVIATE_BACKUP_RESTORE_ID=
# WEAVIATE_BACKUP_HOURLY_FREQUENCY='24'

# ##### #
# REDIS #
# ##### #

# REDIS_URL='redis://redis:6379'
# REDIS_PW='596Lf6'
# REDIS_PORT='6379'

# ############## #
# OpenAI service #
# ############## #

# OPENAI_KEY=
# Below variables let you set specific model versions. The defaults are shown below.
# OPEN_AI_TEXT_GEN_MODEL='gpt-4o'
# OPEN_AI_TEXT_GEN_MINI_MODEL='gpt-4o-mini'
# OPEN_AI_TEXT_GEN_O1_MODEL='o1-preview'
# OPEN_AI_TEXT_GEN_O1_MINI_MODEL='o1-mini'

# #################### #
# Azure OpenAI service #
# #################### #

# AZURE_OPENAI_API_VERSION=

# USING_GPT4O='true'

# Text & embedding generation 
# AZURE_OPENAI_KEY=
# AZURE_OPENAI_RESOURCE=
# AZURE_OPENAI_TXT_DEPLOYMENT=
# AZURE_OPENAI_TXT_DEPLOYMENT_MINI=
# AZURE_OPENAI_EMBED_DEPLOYMENT=

# Vision -> This can be the same values as the text and embedding generation variables but still must be specified
# AZURE_OPENAI_VISION_RESOURCE=
# AZURE_OPENAI_VISION_DEPLOYMENT=
# AZURE_OPENAI_VISION_KEY=

# Image
# AZURE_OPENAI_IMG_RESOURCE=
# AZURE_OPENAI_IMG_DEPLOYMENT=
# AZURE_OPENAI_IMG_KEY=

# Realtime
# AZURE_REALTIME_URL=
# AZURE_REALTIME_KEY=

# o1
# AZURE_OPENAI_O1_KEY=
# AZURE_OPENAI_O1_RESOURCE=
# AZURE_OPENAI_O1_DEPLOYMENT=
# AZURE_OPENAI_O1_MINI_DEPLOYMENT=

# ################ #
# CLAUDE ANTHROPIC #
# ################ #

# CLAUDE_MODEL=
# CLAUDE_API_KEY=

# ########## #
# CLAUDE AWS #
# ########## #

# CLAUDE_AWS_MODEL=
# CLAUDE_AWS_REGION=
# CLAUDE_AWS_ACCESS_KEY=
# CLAUDE_AWS_SECRET_KEY=

# ###### #
# GEMINI #
# ###### #

# GEMINI_MODEL=
# GEMINI_API_KEY=

# ########### #
# VERTEX AUTH #
# ########### #

# VERTEX_PROJECT=
# VERTEX_LOCATION=        

# ############ #
# FILE STORAGE #
# ############ #

# Storage type, can be any of the following: 'azure' | 'aws' | 'google' | 's3-compatible'
# FILE_CLOUD_PROVIDER='s3-compatible'

# 
# Azure storage
# 

# AZURE_STORAGE_CONNECTION_STRING=
# AZURE_STORAGE_ACCOUNT_NAME=
# AZURE_STORAGE_ACCOUNT_KEY=
# AZURE_CONTAINER_NAME=

# 
# AWS S3 Storage
# 

# AWS_REGION=
# AWS_ACCESS_KEY_ID=
# AWS_SECRET_ACCESS_KEY=
# AWS_BUCKET_NAME=

# 
# GOOGLE CLOUD
# 

# GOOGLE_PROJECT_ID=
# GOOGLE_CLIENT_EMAIL=
# GOOGLE_PRIVATE_KEY=
# GOOGLE_BUCKET_NAME=

# 
# s3 compatible - default Minio
# 

# If using Minio or another local s3 service provider, you must set this as localhost (not 0.0.0.0 or 127.0.0.1). Further, when running an s3 provider in Docker as part of the Docker compose setup, you must set the service name in the S3_COMPATIBLE_ENDPOINT_SERVICE env variable -> This is due to networking issues when dealing with services in Docker that need to also be accessed from the web browser. If using cloud-based s3 services, you must set the endpoint to the correct URL and can leave the S3_COMPATIBLE_ENDPOINT_SERVICE commented out.
# S3_COMPATIBLE_ENDPOINT='http://localhost:9000'
# S3_COMPATIBLE_ENDPOINT_SERVICE='minio' 
# S3_COMPATIBLE_ACCESS_KEY_ID='minioadmin'
# S3_COMPATIBLE_SECRET_ACCESS_KEY='minioadmin'
# S3_COMPATIBLE_BUCKET_NAME='academicid'
# S3_COMPATIBLE_FORCE_PATH_STYLE='true'

# ################### #
# Additional Settings #
# ################### #

# SEMANTIC_SCHOLAR_API_KEY=
# PAPERBUZZ_EMAIL=
# BACKEND_PORT='3030'
# FRONTEND_URL='http://localhost:3000'
# BACKEND_URL='http://localhost:3030'
# TIMEZONE='UTC'
# VECTOR_LENGTH='1536' # must be either '1536' or '512'
# CUSTOM_PROMPT=
 
# MAX_WORKERS='3' # sets the number of files that can be processed in parallel (for both the file processing service and research spaces)
 
# ADVANCED_EXTRACTION= # Only set this if you want to extract text from files using the GPT4o vision capabilities; this results in better text extraction and performance in search and RAG, however, it is more expensive as all text is extracted using the GPT4o vision model

# MAX_CONVERSATION_TOKEN_COUNT='124000'
# MAX_OUTPUT_TOKENS='16384' # must be a number less than 16384 (for non-o1 models)
# O1_MAX_RESPONSE_TOKENS='32768'
# O1_MINI_MAX_RESPONSE_TOKENS='65000'
